{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=batch_size).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=batch_size).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(float)\n",
    "target_index = np.ones(batch_size, dtype=int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.484576\n",
      "Epoch 1, loss: 2.356218\n",
      "Epoch 2, loss: 2.318052\n",
      "Epoch 3, loss: 2.306677\n",
      "Epoch 4, loss: 2.303294\n",
      "Epoch 5, loss: 2.302290\n",
      "Epoch 6, loss: 2.301999\n",
      "Epoch 7, loss: 2.301896\n",
      "Epoch 8, loss: 2.301882\n",
      "Epoch 9, loss: 2.301875\n"
     ]
    }
   ],
   "source": [
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2aa1948520>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.127\n",
      "Epoch 0, loss: 2.301881\n",
      "Epoch 1, loss: 2.301861\n",
      "Epoch 2, loss: 2.301883\n",
      "Epoch 3, loss: 2.301850\n",
      "Epoch 4, loss: 2.301866\n",
      "Epoch 5, loss: 2.301856\n",
      "Epoch 6, loss: 2.301871\n",
      "Epoch 7, loss: 2.301882\n",
      "Epoch 8, loss: 2.301863\n",
      "Epoch 9, loss: 2.301867\n",
      "Epoch 10, loss: 2.301850\n",
      "Epoch 11, loss: 2.301870\n",
      "Epoch 12, loss: 2.301859\n",
      "Epoch 13, loss: 2.301856\n",
      "Epoch 14, loss: 2.301873\n",
      "Epoch 15, loss: 2.301870\n",
      "Epoch 16, loss: 2.301878\n",
      "Epoch 17, loss: 2.301869\n",
      "Epoch 18, loss: 2.301863\n",
      "Epoch 19, loss: 2.301873\n",
      "Epoch 20, loss: 2.301862\n",
      "Epoch 21, loss: 2.301860\n",
      "Epoch 22, loss: 2.301864\n",
      "Epoch 23, loss: 2.301856\n",
      "Epoch 24, loss: 2.301859\n",
      "Epoch 25, loss: 2.301868\n",
      "Epoch 26, loss: 2.301854\n",
      "Epoch 27, loss: 2.301854\n",
      "Epoch 28, loss: 2.301862\n",
      "Epoch 29, loss: 2.301861\n",
      "Epoch 30, loss: 2.301866\n",
      "Epoch 31, loss: 2.301873\n",
      "Epoch 32, loss: 2.301840\n",
      "Epoch 33, loss: 2.301873\n",
      "Epoch 34, loss: 2.301870\n",
      "Epoch 35, loss: 2.301873\n",
      "Epoch 36, loss: 2.301862\n",
      "Epoch 37, loss: 2.301852\n",
      "Epoch 38, loss: 2.301882\n",
      "Epoch 39, loss: 2.301852\n",
      "Epoch 40, loss: 2.301866\n",
      "Epoch 41, loss: 2.301850\n",
      "Epoch 42, loss: 2.301880\n",
      "Epoch 43, loss: 2.301866\n",
      "Epoch 44, loss: 2.301872\n",
      "Epoch 45, loss: 2.301854\n",
      "Epoch 46, loss: 2.301856\n",
      "Epoch 47, loss: 2.301868\n",
      "Epoch 48, loss: 2.301858\n",
      "Epoch 49, loss: 2.301874\n",
      "Epoch 50, loss: 2.301853\n",
      "Epoch 51, loss: 2.301868\n",
      "Epoch 52, loss: 2.301866\n",
      "Epoch 53, loss: 2.301867\n",
      "Epoch 54, loss: 2.301855\n",
      "Epoch 55, loss: 2.301868\n",
      "Epoch 56, loss: 2.301853\n",
      "Epoch 57, loss: 2.301865\n",
      "Epoch 58, loss: 2.301878\n",
      "Epoch 59, loss: 2.301843\n",
      "Epoch 60, loss: 2.301875\n",
      "Epoch 61, loss: 2.301862\n",
      "Epoch 62, loss: 2.301871\n",
      "Epoch 63, loss: 2.301872\n",
      "Epoch 64, loss: 2.301879\n",
      "Epoch 65, loss: 2.301859\n",
      "Epoch 66, loss: 2.301867\n",
      "Epoch 67, loss: 2.301863\n",
      "Epoch 68, loss: 2.301868\n",
      "Epoch 69, loss: 2.301886\n",
      "Epoch 70, loss: 2.301880\n",
      "Epoch 71, loss: 2.301870\n",
      "Epoch 72, loss: 2.301861\n",
      "Epoch 73, loss: 2.301867\n",
      "Epoch 74, loss: 2.301865\n",
      "Epoch 75, loss: 2.301847\n",
      "Epoch 76, loss: 2.301863\n",
      "Epoch 77, loss: 2.301873\n",
      "Epoch 78, loss: 2.301887\n",
      "Epoch 79, loss: 2.301867\n",
      "Epoch 80, loss: 2.301881\n",
      "Epoch 81, loss: 2.301860\n",
      "Epoch 82, loss: 2.301853\n",
      "Epoch 83, loss: 2.301877\n",
      "Epoch 84, loss: 2.301868\n",
      "Epoch 85, loss: 2.301854\n",
      "Epoch 86, loss: 2.301871\n",
      "Epoch 87, loss: 2.301861\n",
      "Epoch 88, loss: 2.301856\n",
      "Epoch 89, loss: 2.301875\n",
      "Epoch 90, loss: 2.301854\n",
      "Epoch 91, loss: 2.301853\n",
      "Epoch 92, loss: 2.301876\n",
      "Epoch 93, loss: 2.301862\n",
      "Epoch 94, loss: 2.301873\n",
      "Epoch 95, loss: 2.301864\n",
      "Epoch 96, loss: 2.301875\n",
      "Epoch 97, loss: 2.301872\n",
      "Epoch 98, loss: 2.301859\n",
      "Epoch 99, loss: 2.301866\n",
      "Accuracy after training for 100 epochs:  0.125\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate = 0.001000\n",
      "Reg strength = 0.000100\n",
      "Epoch 0, loss: 2.302203\n",
      "Epoch 1, loss: 2.301131\n",
      "Epoch 2, loss: 2.300136\n",
      "Epoch 3, loss: 2.299179\n",
      "Epoch 4, loss: 2.298262\n",
      "Epoch 5, loss: 2.297359\n",
      "Epoch 6, loss: 2.296483\n",
      "Epoch 7, loss: 2.295598\n",
      "Epoch 8, loss: 2.294735\n",
      "Epoch 9, loss: 2.293872\n",
      "Epoch 10, loss: 2.293035\n",
      "Epoch 11, loss: 2.292200\n",
      "Epoch 12, loss: 2.291386\n",
      "Epoch 13, loss: 2.290546\n",
      "Epoch 14, loss: 2.289726\n",
      "Epoch 15, loss: 2.288930\n",
      "Epoch 16, loss: 2.288139\n",
      "Epoch 17, loss: 2.287353\n",
      "Epoch 18, loss: 2.286556\n",
      "Epoch 19, loss: 2.285764\n",
      "Epoch 20, loss: 2.284988\n",
      "Epoch 21, loss: 2.284220\n",
      "Epoch 22, loss: 2.283454\n",
      "Epoch 23, loss: 2.282701\n",
      "Epoch 24, loss: 2.281940\n",
      "Epoch 25, loss: 2.281195\n",
      "Epoch 26, loss: 2.280451\n",
      "Epoch 27, loss: 2.279732\n",
      "Epoch 28, loss: 2.279008\n",
      "Epoch 29, loss: 2.278273\n",
      "Epoch 30, loss: 2.277557\n",
      "Epoch 31, loss: 2.276822\n",
      "Epoch 32, loss: 2.276129\n",
      "Epoch 33, loss: 2.275417\n",
      "Epoch 34, loss: 2.274708\n",
      "Epoch 35, loss: 2.274033\n",
      "Epoch 36, loss: 2.273357\n",
      "Epoch 37, loss: 2.272669\n",
      "Epoch 38, loss: 2.271969\n",
      "Epoch 39, loss: 2.271313\n",
      "Epoch 40, loss: 2.270618\n",
      "Epoch 41, loss: 2.269957\n",
      "Epoch 42, loss: 2.269315\n",
      "Epoch 43, loss: 2.268647\n",
      "Epoch 44, loss: 2.267991\n",
      "Epoch 45, loss: 2.267336\n",
      "Epoch 46, loss: 2.266698\n",
      "Epoch 47, loss: 2.266070\n",
      "Epoch 48, loss: 2.265417\n",
      "Epoch 49, loss: 2.264780\n",
      "Epoch 50, loss: 2.264173\n",
      "Epoch 51, loss: 2.263551\n",
      "Epoch 52, loss: 2.262916\n",
      "Epoch 53, loss: 2.262299\n",
      "Epoch 54, loss: 2.261697\n",
      "Epoch 55, loss: 2.261108\n",
      "Epoch 56, loss: 2.260490\n",
      "Epoch 57, loss: 2.259883\n",
      "Epoch 58, loss: 2.259292\n",
      "Epoch 59, loss: 2.258701\n",
      "Epoch 60, loss: 2.258115\n",
      "Epoch 61, loss: 2.257528\n",
      "Epoch 62, loss: 2.256945\n",
      "Epoch 63, loss: 2.256387\n",
      "Epoch 64, loss: 2.255815\n",
      "Epoch 65, loss: 2.255247\n",
      "Epoch 66, loss: 2.254649\n",
      "Epoch 67, loss: 2.254122\n",
      "Epoch 68, loss: 2.253534\n",
      "Epoch 69, loss: 2.253001\n",
      "Epoch 70, loss: 2.252431\n",
      "Epoch 71, loss: 2.251892\n",
      "Epoch 72, loss: 2.251362\n",
      "Epoch 73, loss: 2.250786\n",
      "Epoch 74, loss: 2.250253\n",
      "Epoch 75, loss: 2.249718\n",
      "Epoch 76, loss: 2.249184\n",
      "Epoch 77, loss: 2.248666\n",
      "Epoch 78, loss: 2.248128\n",
      "Epoch 79, loss: 2.247605\n",
      "Epoch 80, loss: 2.247090\n",
      "Epoch 81, loss: 2.246553\n",
      "Epoch 82, loss: 2.246038\n",
      "Epoch 83, loss: 2.245545\n",
      "Epoch 84, loss: 2.245048\n",
      "Epoch 85, loss: 2.244546\n",
      "Epoch 86, loss: 2.244014\n",
      "Epoch 87, loss: 2.243539\n",
      "Epoch 88, loss: 2.243035\n",
      "Epoch 89, loss: 2.242551\n",
      "Epoch 90, loss: 2.242055\n",
      "Epoch 91, loss: 2.241567\n",
      "Epoch 92, loss: 2.241086\n",
      "Epoch 93, loss: 2.240589\n",
      "Epoch 94, loss: 2.240120\n",
      "Epoch 95, loss: 2.239643\n",
      "Epoch 96, loss: 2.239163\n",
      "Epoch 97, loss: 2.238708\n",
      "Epoch 98, loss: 2.238243\n",
      "Epoch 99, loss: 2.237767\n",
      "Epoch 100, loss: 2.237297\n",
      "Epoch 101, loss: 2.236863\n",
      "Epoch 102, loss: 2.236381\n",
      "Epoch 103, loss: 2.235934\n",
      "Epoch 104, loss: 2.235504\n",
      "Epoch 105, loss: 2.235020\n",
      "Epoch 106, loss: 2.234557\n",
      "Epoch 107, loss: 2.234155\n",
      "Epoch 108, loss: 2.233699\n",
      "Epoch 109, loss: 2.233262\n",
      "Epoch 110, loss: 2.232833\n",
      "Epoch 111, loss: 2.232382\n",
      "Epoch 112, loss: 2.231950\n",
      "Epoch 113, loss: 2.231538\n",
      "Epoch 114, loss: 2.231123\n",
      "Epoch 115, loss: 2.230682\n",
      "Epoch 116, loss: 2.230251\n",
      "Epoch 117, loss: 2.229854\n",
      "Epoch 118, loss: 2.229414\n",
      "Epoch 119, loss: 2.229005\n",
      "Epoch 120, loss: 2.228577\n",
      "Epoch 121, loss: 2.228196\n",
      "Epoch 122, loss: 2.227759\n",
      "Epoch 123, loss: 2.227382\n",
      "Epoch 124, loss: 2.226954\n",
      "Epoch 125, loss: 2.226560\n",
      "Epoch 126, loss: 2.226157\n",
      "Epoch 127, loss: 2.225748\n",
      "Epoch 128, loss: 2.225358\n",
      "Epoch 129, loss: 2.224973\n",
      "Epoch 130, loss: 2.224578\n",
      "Epoch 131, loss: 2.224204\n",
      "Epoch 132, loss: 2.223798\n",
      "Epoch 133, loss: 2.223408\n",
      "Epoch 134, loss: 2.223034\n",
      "Epoch 135, loss: 2.222672\n",
      "Epoch 136, loss: 2.222282\n",
      "Epoch 137, loss: 2.221909\n",
      "Epoch 138, loss: 2.221555\n",
      "Epoch 139, loss: 2.221151\n",
      "Epoch 140, loss: 2.220792\n",
      "Epoch 141, loss: 2.220424\n",
      "Epoch 142, loss: 2.220045\n",
      "Epoch 143, loss: 2.219686\n",
      "Epoch 144, loss: 2.219327\n",
      "Epoch 145, loss: 2.218965\n",
      "Epoch 146, loss: 2.218604\n",
      "Epoch 147, loss: 2.218260\n",
      "Epoch 148, loss: 2.217893\n",
      "Epoch 149, loss: 2.217560\n",
      "Epoch 150, loss: 2.217201\n",
      "Epoch 151, loss: 2.216843\n",
      "Epoch 152, loss: 2.216521\n",
      "Epoch 153, loss: 2.216162\n",
      "Epoch 154, loss: 2.215812\n",
      "Epoch 155, loss: 2.215484\n",
      "Epoch 156, loss: 2.215127\n",
      "Epoch 157, loss: 2.214792\n",
      "Epoch 158, loss: 2.214475\n",
      "Epoch 159, loss: 2.214134\n",
      "Epoch 160, loss: 2.213795\n",
      "Epoch 161, loss: 2.213466\n",
      "Epoch 162, loss: 2.213151\n",
      "Epoch 163, loss: 2.212829\n",
      "Epoch 164, loss: 2.212480\n",
      "Epoch 165, loss: 2.212170\n",
      "Epoch 166, loss: 2.211849\n",
      "Epoch 167, loss: 2.211534\n",
      "Epoch 168, loss: 2.211217\n",
      "Epoch 169, loss: 2.210880\n",
      "Epoch 170, loss: 2.210566\n",
      "Epoch 171, loss: 2.210252\n",
      "Epoch 172, loss: 2.209934\n",
      "Epoch 173, loss: 2.209648\n",
      "Epoch 174, loss: 2.209335\n",
      "Epoch 175, loss: 2.209027\n",
      "Epoch 176, loss: 2.208708\n",
      "Epoch 177, loss: 2.208427\n",
      "Epoch 178, loss: 2.208119\n",
      "Epoch 179, loss: 2.207808\n",
      "Epoch 180, loss: 2.207495\n",
      "Epoch 181, loss: 2.207213\n",
      "Epoch 182, loss: 2.206932\n",
      "Epoch 183, loss: 2.206624\n",
      "Epoch 184, loss: 2.206336\n",
      "Epoch 185, loss: 2.206030\n",
      "Epoch 186, loss: 2.205743\n",
      "Epoch 187, loss: 2.205461\n",
      "Epoch 188, loss: 2.205169\n",
      "Epoch 189, loss: 2.204882\n",
      "Epoch 190, loss: 2.204603\n",
      "Epoch 191, loss: 2.204318\n",
      "Epoch 192, loss: 2.204035\n",
      "Epoch 193, loss: 2.203765\n",
      "Epoch 194, loss: 2.203476\n",
      "Epoch 195, loss: 2.203202\n",
      "Epoch 196, loss: 2.202939\n",
      "Epoch 197, loss: 2.202639\n",
      "Epoch 198, loss: 2.202388\n",
      "Epoch 199, loss: 2.202106\n",
      "Accuracy = 0.229000\n",
      "\n",
      "Learning rate = 0.001000\n",
      "Reg strength = 0.000010\n",
      "Epoch 0, loss: 2.302589\n",
      "Epoch 1, loss: 2.301548\n",
      "Epoch 2, loss: 2.300569\n",
      "Epoch 3, loss: 2.299607\n",
      "Epoch 4, loss: 2.298665\n",
      "Epoch 5, loss: 2.297771\n",
      "Epoch 6, loss: 2.296863\n",
      "Epoch 7, loss: 2.295979\n",
      "Epoch 8, loss: 2.295129\n",
      "Epoch 9, loss: 2.294270\n",
      "Epoch 10, loss: 2.293414\n",
      "Epoch 11, loss: 2.292572\n",
      "Epoch 12, loss: 2.291748\n",
      "Epoch 13, loss: 2.290918\n",
      "Epoch 14, loss: 2.290109\n",
      "Epoch 15, loss: 2.289306\n",
      "Epoch 16, loss: 2.288487\n",
      "Epoch 17, loss: 2.287695\n",
      "Epoch 18, loss: 2.286899\n",
      "Epoch 19, loss: 2.286128\n",
      "Epoch 20, loss: 2.285341\n",
      "Epoch 21, loss: 2.284575\n",
      "Epoch 22, loss: 2.283801\n",
      "Epoch 23, loss: 2.283036\n",
      "Epoch 24, loss: 2.282274\n",
      "Epoch 25, loss: 2.281537\n",
      "Epoch 26, loss: 2.280780\n",
      "Epoch 27, loss: 2.280046\n",
      "Epoch 28, loss: 2.279324\n",
      "Epoch 29, loss: 2.278577\n",
      "Epoch 30, loss: 2.277856\n",
      "Epoch 31, loss: 2.277138\n",
      "Epoch 32, loss: 2.276434\n",
      "Epoch 33, loss: 2.275714\n",
      "Epoch 34, loss: 2.275037\n",
      "Epoch 35, loss: 2.274325\n",
      "Epoch 36, loss: 2.273637\n",
      "Epoch 37, loss: 2.272953\n",
      "Epoch 38, loss: 2.272269\n",
      "Epoch 39, loss: 2.271582\n",
      "Epoch 40, loss: 2.270910\n",
      "Epoch 41, loss: 2.270244\n",
      "Epoch 42, loss: 2.269582\n",
      "Epoch 43, loss: 2.268923\n",
      "Epoch 44, loss: 2.268286\n",
      "Epoch 45, loss: 2.267611\n",
      "Epoch 46, loss: 2.266975\n",
      "Epoch 47, loss: 2.266347\n",
      "Epoch 48, loss: 2.265704\n",
      "Epoch 49, loss: 2.265088\n",
      "Epoch 50, loss: 2.264429\n",
      "Epoch 51, loss: 2.263788\n",
      "Epoch 52, loss: 2.263184\n",
      "Epoch 53, loss: 2.262571\n",
      "Epoch 54, loss: 2.261945\n",
      "Epoch 55, loss: 2.261351\n",
      "Epoch 56, loss: 2.260763\n",
      "Epoch 57, loss: 2.260138\n",
      "Epoch 58, loss: 2.259550\n",
      "Epoch 59, loss: 2.258940\n",
      "Epoch 60, loss: 2.258356\n",
      "Epoch 61, loss: 2.257782\n",
      "Epoch 62, loss: 2.257178\n",
      "Epoch 63, loss: 2.256614\n",
      "Epoch 64, loss: 2.256044\n",
      "Epoch 65, loss: 2.255472\n",
      "Epoch 66, loss: 2.254903\n",
      "Epoch 67, loss: 2.254332\n",
      "Epoch 68, loss: 2.253770\n",
      "Epoch 69, loss: 2.253209\n",
      "Epoch 70, loss: 2.252681\n",
      "Epoch 71, loss: 2.252102\n",
      "Epoch 72, loss: 2.251568\n",
      "Epoch 73, loss: 2.251022\n",
      "Epoch 74, loss: 2.250462\n",
      "Epoch 75, loss: 2.249930\n",
      "Epoch 76, loss: 2.249421\n",
      "Epoch 77, loss: 2.248875\n",
      "Epoch 78, loss: 2.248365\n",
      "Epoch 79, loss: 2.247820\n",
      "Epoch 80, loss: 2.247287\n",
      "Epoch 81, loss: 2.246776\n",
      "Epoch 82, loss: 2.246276\n",
      "Epoch 83, loss: 2.245750\n",
      "Epoch 84, loss: 2.245242\n",
      "Epoch 85, loss: 2.244739\n",
      "Epoch 86, loss: 2.244228\n",
      "Epoch 87, loss: 2.243745\n",
      "Epoch 88, loss: 2.243226\n",
      "Epoch 89, loss: 2.242717\n",
      "Epoch 90, loss: 2.242215\n",
      "Epoch 91, loss: 2.241758\n",
      "Epoch 92, loss: 2.241280\n",
      "Epoch 93, loss: 2.240782\n",
      "Epoch 94, loss: 2.240297\n",
      "Epoch 95, loss: 2.239815\n",
      "Epoch 96, loss: 2.239343\n",
      "Epoch 97, loss: 2.238895\n",
      "Epoch 98, loss: 2.238412\n",
      "Epoch 99, loss: 2.237942\n",
      "Epoch 100, loss: 2.237487\n",
      "Epoch 101, loss: 2.237010\n",
      "Epoch 102, loss: 2.236562\n",
      "Epoch 103, loss: 2.236096\n",
      "Epoch 104, loss: 2.235645\n",
      "Epoch 105, loss: 2.235196\n",
      "Epoch 106, loss: 2.234742\n",
      "Epoch 107, loss: 2.234309\n",
      "Epoch 108, loss: 2.233882\n",
      "Epoch 109, loss: 2.233429\n",
      "Epoch 110, loss: 2.232995\n",
      "Epoch 111, loss: 2.232529\n",
      "Epoch 112, loss: 2.232119\n",
      "Epoch 113, loss: 2.231659\n",
      "Epoch 114, loss: 2.231255\n",
      "Epoch 115, loss: 2.230820\n",
      "Epoch 116, loss: 2.230397\n",
      "Epoch 117, loss: 2.229993\n",
      "Epoch 118, loss: 2.229554\n",
      "Epoch 119, loss: 2.229134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, loss: 2.228714\n",
      "Epoch 121, loss: 2.228298\n",
      "Epoch 122, loss: 2.227912\n",
      "Epoch 123, loss: 2.227494\n",
      "Epoch 124, loss: 2.227114\n",
      "Epoch 125, loss: 2.226688\n",
      "Epoch 126, loss: 2.226295\n",
      "Epoch 127, loss: 2.225882\n",
      "Epoch 128, loss: 2.225503\n",
      "Epoch 129, loss: 2.225115\n",
      "Epoch 130, loss: 2.224703\n",
      "Epoch 131, loss: 2.224314\n",
      "Epoch 132, loss: 2.223927\n",
      "Epoch 133, loss: 2.223557\n",
      "Epoch 134, loss: 2.223137\n",
      "Epoch 135, loss: 2.222775\n",
      "Epoch 136, loss: 2.222401\n",
      "Epoch 137, loss: 2.222025\n",
      "Epoch 138, loss: 2.221637\n",
      "Epoch 139, loss: 2.221271\n",
      "Epoch 140, loss: 2.220882\n",
      "Epoch 141, loss: 2.220526\n",
      "Epoch 142, loss: 2.220156\n",
      "Epoch 143, loss: 2.219800\n",
      "Epoch 144, loss: 2.219444\n",
      "Epoch 145, loss: 2.219072\n",
      "Epoch 146, loss: 2.218733\n",
      "Epoch 147, loss: 2.218354\n",
      "Epoch 148, loss: 2.218007\n",
      "Epoch 149, loss: 2.217655\n",
      "Epoch 150, loss: 2.217292\n",
      "Epoch 151, loss: 2.216942\n",
      "Epoch 152, loss: 2.216624\n",
      "Epoch 153, loss: 2.216256\n",
      "Epoch 154, loss: 2.215899\n",
      "Epoch 155, loss: 2.215566\n",
      "Epoch 156, loss: 2.215221\n",
      "Epoch 157, loss: 2.214903\n",
      "Epoch 158, loss: 2.214576\n",
      "Epoch 159, loss: 2.214208\n",
      "Epoch 160, loss: 2.213881\n",
      "Epoch 161, loss: 2.213548\n",
      "Epoch 162, loss: 2.213226\n",
      "Epoch 163, loss: 2.212894\n",
      "Epoch 164, loss: 2.212564\n",
      "Epoch 165, loss: 2.212246\n",
      "Epoch 166, loss: 2.211931\n",
      "Epoch 167, loss: 2.211592\n",
      "Epoch 168, loss: 2.211265\n",
      "Epoch 169, loss: 2.210958\n",
      "Epoch 170, loss: 2.210622\n",
      "Epoch 171, loss: 2.210329\n",
      "Epoch 172, loss: 2.210015\n",
      "Epoch 173, loss: 2.209702\n",
      "Epoch 174, loss: 2.209400\n",
      "Epoch 175, loss: 2.209076\n",
      "Epoch 176, loss: 2.208771\n",
      "Epoch 177, loss: 2.208468\n",
      "Epoch 178, loss: 2.208173\n",
      "Epoch 179, loss: 2.207881\n",
      "Epoch 180, loss: 2.207573\n",
      "Epoch 181, loss: 2.207267\n",
      "Epoch 182, loss: 2.206957\n",
      "Epoch 183, loss: 2.206679\n",
      "Epoch 184, loss: 2.206379\n",
      "Epoch 185, loss: 2.206104\n",
      "Epoch 186, loss: 2.205783\n",
      "Epoch 187, loss: 2.205503\n",
      "Epoch 188, loss: 2.205205\n",
      "Epoch 189, loss: 2.204934\n",
      "Epoch 190, loss: 2.204657\n",
      "Epoch 191, loss: 2.204381\n",
      "Epoch 192, loss: 2.204078\n",
      "Epoch 193, loss: 2.203801\n",
      "Epoch 194, loss: 2.203531\n",
      "Epoch 195, loss: 2.203255\n",
      "Epoch 196, loss: 2.202957\n",
      "Epoch 197, loss: 2.202681\n",
      "Epoch 198, loss: 2.202426\n",
      "Epoch 199, loss: 2.202147\n",
      "Accuracy = 0.228000\n",
      "\n",
      "Learning rate = 0.001000\n",
      "Reg strength = 0.000001\n",
      "Epoch 0, loss: 2.302111\n",
      "Epoch 1, loss: 2.301059\n",
      "Epoch 2, loss: 2.300091\n",
      "Epoch 3, loss: 2.299169\n",
      "Epoch 4, loss: 2.298225\n",
      "Epoch 5, loss: 2.297336\n",
      "Epoch 6, loss: 2.296458\n",
      "Epoch 7, loss: 2.295546\n",
      "Epoch 8, loss: 2.294707\n",
      "Epoch 9, loss: 2.293856\n",
      "Epoch 10, loss: 2.293001\n",
      "Epoch 11, loss: 2.292175\n",
      "Epoch 12, loss: 2.291340\n",
      "Epoch 13, loss: 2.290518\n",
      "Epoch 14, loss: 2.289701\n",
      "Epoch 15, loss: 2.288910\n",
      "Epoch 16, loss: 2.288086\n",
      "Epoch 17, loss: 2.287308\n",
      "Epoch 18, loss: 2.286522\n",
      "Epoch 19, loss: 2.285717\n",
      "Epoch 20, loss: 2.284945\n",
      "Epoch 21, loss: 2.284186\n",
      "Epoch 22, loss: 2.283418\n",
      "Epoch 23, loss: 2.282667\n",
      "Epoch 24, loss: 2.281901\n",
      "Epoch 25, loss: 2.281153\n",
      "Epoch 26, loss: 2.280419\n",
      "Epoch 27, loss: 2.279695\n",
      "Epoch 28, loss: 2.278960\n",
      "Epoch 29, loss: 2.278271\n",
      "Epoch 30, loss: 2.277504\n",
      "Epoch 31, loss: 2.276803\n",
      "Epoch 32, loss: 2.276079\n",
      "Epoch 33, loss: 2.275399\n",
      "Epoch 34, loss: 2.274688\n",
      "Epoch 35, loss: 2.273989\n",
      "Epoch 36, loss: 2.273300\n",
      "Epoch 37, loss: 2.272630\n",
      "Epoch 38, loss: 2.271955\n",
      "Epoch 39, loss: 2.271275\n",
      "Epoch 40, loss: 2.270594\n",
      "Epoch 41, loss: 2.269915\n",
      "Epoch 42, loss: 2.269252\n",
      "Epoch 43, loss: 2.268610\n",
      "Epoch 44, loss: 2.267958\n",
      "Epoch 45, loss: 2.267309\n",
      "Epoch 46, loss: 2.266664\n",
      "Epoch 47, loss: 2.266017\n",
      "Epoch 48, loss: 2.265394\n",
      "Epoch 49, loss: 2.264761\n",
      "Epoch 50, loss: 2.264135\n",
      "Epoch 51, loss: 2.263497\n",
      "Epoch 52, loss: 2.262890\n",
      "Epoch 53, loss: 2.262268\n",
      "Epoch 54, loss: 2.261669\n",
      "Epoch 55, loss: 2.261071\n",
      "Epoch 56, loss: 2.260454\n",
      "Epoch 57, loss: 2.259833\n",
      "Epoch 58, loss: 2.259258\n",
      "Epoch 59, loss: 2.258675\n",
      "Epoch 60, loss: 2.258083\n",
      "Epoch 61, loss: 2.257494\n",
      "Epoch 62, loss: 2.256907\n",
      "Epoch 63, loss: 2.256332\n",
      "Epoch 64, loss: 2.255760\n",
      "Epoch 65, loss: 2.255183\n",
      "Epoch 66, loss: 2.254624\n",
      "Epoch 67, loss: 2.254071\n",
      "Epoch 68, loss: 2.253508\n",
      "Epoch 69, loss: 2.252948\n",
      "Epoch 70, loss: 2.252402\n",
      "Epoch 71, loss: 2.251843\n",
      "Epoch 72, loss: 2.251299\n",
      "Epoch 73, loss: 2.250767\n",
      "Epoch 74, loss: 2.250234\n",
      "Epoch 75, loss: 2.249680\n",
      "Epoch 76, loss: 2.249147\n",
      "Epoch 77, loss: 2.248618\n",
      "Epoch 78, loss: 2.248087\n",
      "Epoch 79, loss: 2.247579\n",
      "Epoch 80, loss: 2.247050\n",
      "Epoch 81, loss: 2.246529\n",
      "Epoch 82, loss: 2.246013\n",
      "Epoch 83, loss: 2.245500\n",
      "Epoch 84, loss: 2.244995\n",
      "Epoch 85, loss: 2.244490\n",
      "Epoch 86, loss: 2.243976\n",
      "Epoch 87, loss: 2.243498\n",
      "Epoch 88, loss: 2.243000\n",
      "Epoch 89, loss: 2.242505\n",
      "Epoch 90, loss: 2.242024\n",
      "Epoch 91, loss: 2.241525\n",
      "Epoch 92, loss: 2.241026\n",
      "Epoch 93, loss: 2.240565\n",
      "Epoch 94, loss: 2.240095\n",
      "Epoch 95, loss: 2.239614\n",
      "Epoch 96, loss: 2.239136\n",
      "Epoch 97, loss: 2.238664\n",
      "Epoch 98, loss: 2.238195\n",
      "Epoch 99, loss: 2.237725\n",
      "Epoch 100, loss: 2.237260\n",
      "Epoch 101, loss: 2.236819\n",
      "Epoch 102, loss: 2.236345\n",
      "Epoch 103, loss: 2.235886\n",
      "Epoch 104, loss: 2.235443\n",
      "Epoch 105, loss: 2.234977\n",
      "Epoch 106, loss: 2.234532\n",
      "Epoch 107, loss: 2.234114\n",
      "Epoch 108, loss: 2.233658\n",
      "Epoch 109, loss: 2.233225\n",
      "Epoch 110, loss: 2.232773\n",
      "Epoch 111, loss: 2.232346\n",
      "Epoch 112, loss: 2.231909\n",
      "Epoch 113, loss: 2.231460\n",
      "Epoch 114, loss: 2.231049\n",
      "Epoch 115, loss: 2.230618\n",
      "Epoch 116, loss: 2.230210\n",
      "Epoch 117, loss: 2.229786\n",
      "Epoch 118, loss: 2.229362\n",
      "Epoch 119, loss: 2.228941\n",
      "Epoch 120, loss: 2.228542\n",
      "Epoch 121, loss: 2.228106\n",
      "Epoch 122, loss: 2.227698\n",
      "Epoch 123, loss: 2.227317\n",
      "Epoch 124, loss: 2.226904\n",
      "Epoch 125, loss: 2.226498\n",
      "Epoch 126, loss: 2.226103\n",
      "Epoch 127, loss: 2.225703\n",
      "Epoch 128, loss: 2.225316\n",
      "Epoch 129, loss: 2.224925\n",
      "Epoch 130, loss: 2.224534\n",
      "Epoch 131, loss: 2.224126\n",
      "Epoch 132, loss: 2.223749\n",
      "Epoch 133, loss: 2.223376\n",
      "Epoch 134, loss: 2.223014\n",
      "Epoch 135, loss: 2.222603\n",
      "Epoch 136, loss: 2.222228\n",
      "Epoch 137, loss: 2.221853\n",
      "Epoch 138, loss: 2.221466\n",
      "Epoch 139, loss: 2.221104\n",
      "Epoch 140, loss: 2.220732\n",
      "Epoch 141, loss: 2.220371\n",
      "Epoch 142, loss: 2.219988\n",
      "Epoch 143, loss: 2.219637\n",
      "Epoch 144, loss: 2.219286\n",
      "Epoch 145, loss: 2.218909\n",
      "Epoch 146, loss: 2.218555\n",
      "Epoch 147, loss: 2.218205\n",
      "Epoch 148, loss: 2.217854\n",
      "Epoch 149, loss: 2.217495\n",
      "Epoch 150, loss: 2.217123\n",
      "Epoch 151, loss: 2.216792\n",
      "Epoch 152, loss: 2.216451\n",
      "Epoch 153, loss: 2.216106\n",
      "Epoch 154, loss: 2.215743\n",
      "Epoch 155, loss: 2.215402\n",
      "Epoch 156, loss: 2.215063\n",
      "Epoch 157, loss: 2.214727\n",
      "Epoch 158, loss: 2.214406\n",
      "Epoch 159, loss: 2.214067\n",
      "Epoch 160, loss: 2.213732\n",
      "Epoch 161, loss: 2.213413\n",
      "Epoch 162, loss: 2.213065\n",
      "Epoch 163, loss: 2.212751\n",
      "Epoch 164, loss: 2.212434\n",
      "Epoch 165, loss: 2.212091\n",
      "Epoch 166, loss: 2.211761\n",
      "Epoch 167, loss: 2.211448\n",
      "Epoch 168, loss: 2.211138\n",
      "Epoch 169, loss: 2.210824\n",
      "Epoch 170, loss: 2.210493\n",
      "Epoch 171, loss: 2.210194\n",
      "Epoch 172, loss: 2.209867\n",
      "Epoch 173, loss: 2.209589\n",
      "Epoch 174, loss: 2.209254\n",
      "Epoch 175, loss: 2.208951\n",
      "Epoch 176, loss: 2.208637\n",
      "Epoch 177, loss: 2.208343\n",
      "Epoch 178, loss: 2.208024\n",
      "Epoch 179, loss: 2.207741\n",
      "Epoch 180, loss: 2.207448\n",
      "Epoch 181, loss: 2.207136\n",
      "Epoch 182, loss: 2.206834\n",
      "Epoch 183, loss: 2.206551\n",
      "Epoch 184, loss: 2.206242\n",
      "Epoch 185, loss: 2.205959\n",
      "Epoch 186, loss: 2.205657\n",
      "Epoch 187, loss: 2.205383\n",
      "Epoch 188, loss: 2.205104\n",
      "Epoch 189, loss: 2.204806\n",
      "Epoch 190, loss: 2.204507\n",
      "Epoch 191, loss: 2.204237\n",
      "Epoch 192, loss: 2.203947\n",
      "Epoch 193, loss: 2.203694\n",
      "Epoch 194, loss: 2.203399\n",
      "Epoch 195, loss: 2.203112\n",
      "Epoch 196, loss: 2.202844\n",
      "Epoch 197, loss: 2.202565\n",
      "Epoch 198, loss: 2.202299\n",
      "Epoch 199, loss: 2.202029\n",
      "Accuracy = 0.227000\n",
      "\n",
      "Learning rate = 0.000100\n",
      "Reg strength = 0.000100\n",
      "Epoch 0, loss: 2.302706\n",
      "Epoch 1, loss: 2.302598\n",
      "Epoch 2, loss: 2.302490\n",
      "Epoch 3, loss: 2.302383\n",
      "Epoch 4, loss: 2.302278\n",
      "Epoch 5, loss: 2.302173\n",
      "Epoch 6, loss: 2.302068\n",
      "Epoch 7, loss: 2.301963\n",
      "Epoch 8, loss: 2.301863\n",
      "Epoch 9, loss: 2.301759\n",
      "Epoch 10, loss: 2.301656\n",
      "Epoch 11, loss: 2.301555\n",
      "Epoch 12, loss: 2.301456\n",
      "Epoch 13, loss: 2.301355\n",
      "Epoch 14, loss: 2.301252\n",
      "Epoch 15, loss: 2.301153\n",
      "Epoch 16, loss: 2.301053\n",
      "Epoch 17, loss: 2.300956\n",
      "Epoch 18, loss: 2.300856\n",
      "Epoch 19, loss: 2.300762\n",
      "Epoch 20, loss: 2.300662\n",
      "Epoch 21, loss: 2.300563\n",
      "Epoch 22, loss: 2.300466\n",
      "Epoch 23, loss: 2.300371\n",
      "Epoch 24, loss: 2.300275\n",
      "Epoch 25, loss: 2.300179\n",
      "Epoch 26, loss: 2.300082\n",
      "Epoch 27, loss: 2.299988\n",
      "Epoch 28, loss: 2.299893\n",
      "Epoch 29, loss: 2.299796\n",
      "Epoch 30, loss: 2.299705\n",
      "Epoch 31, loss: 2.299609\n",
      "Epoch 32, loss: 2.299518\n",
      "Epoch 33, loss: 2.299421\n",
      "Epoch 34, loss: 2.299330\n",
      "Epoch 35, loss: 2.299235\n",
      "Epoch 36, loss: 2.299144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, loss: 2.299052\n",
      "Epoch 38, loss: 2.298958\n",
      "Epoch 39, loss: 2.298866\n",
      "Epoch 40, loss: 2.298775\n",
      "Epoch 41, loss: 2.298682\n",
      "Epoch 42, loss: 2.298591\n",
      "Epoch 43, loss: 2.298501\n",
      "Epoch 44, loss: 2.298410\n",
      "Epoch 45, loss: 2.298317\n",
      "Epoch 46, loss: 2.298227\n",
      "Epoch 47, loss: 2.298136\n",
      "Epoch 48, loss: 2.298047\n",
      "Epoch 49, loss: 2.297956\n",
      "Epoch 50, loss: 2.297865\n",
      "Epoch 51, loss: 2.297776\n",
      "Epoch 52, loss: 2.297688\n",
      "Epoch 53, loss: 2.297596\n",
      "Epoch 54, loss: 2.297507\n",
      "Epoch 55, loss: 2.297419\n",
      "Epoch 56, loss: 2.297326\n",
      "Epoch 57, loss: 2.297238\n",
      "Epoch 58, loss: 2.297149\n",
      "Epoch 59, loss: 2.297060\n",
      "Epoch 60, loss: 2.296974\n",
      "Epoch 61, loss: 2.296883\n",
      "Epoch 62, loss: 2.296796\n",
      "Epoch 63, loss: 2.296708\n",
      "Epoch 64, loss: 2.296620\n",
      "Epoch 65, loss: 2.296531\n",
      "Epoch 66, loss: 2.296443\n",
      "Epoch 67, loss: 2.296357\n",
      "Epoch 68, loss: 2.296269\n",
      "Epoch 69, loss: 2.296181\n",
      "Epoch 70, loss: 2.296093\n",
      "Epoch 71, loss: 2.296006\n",
      "Epoch 72, loss: 2.295918\n",
      "Epoch 73, loss: 2.295832\n",
      "Epoch 74, loss: 2.295745\n",
      "Epoch 75, loss: 2.295659\n",
      "Epoch 76, loss: 2.295569\n",
      "Epoch 77, loss: 2.295485\n",
      "Epoch 78, loss: 2.295397\n",
      "Epoch 79, loss: 2.295313\n",
      "Epoch 80, loss: 2.295225\n",
      "Epoch 81, loss: 2.295139\n",
      "Epoch 82, loss: 2.295051\n",
      "Epoch 83, loss: 2.294967\n",
      "Epoch 84, loss: 2.294881\n",
      "Epoch 85, loss: 2.294794\n",
      "Epoch 86, loss: 2.294711\n",
      "Epoch 87, loss: 2.294626\n",
      "Epoch 88, loss: 2.294538\n",
      "Epoch 89, loss: 2.294453\n",
      "Epoch 90, loss: 2.294367\n",
      "Epoch 91, loss: 2.294282\n",
      "Epoch 92, loss: 2.294197\n",
      "Epoch 93, loss: 2.294110\n",
      "Epoch 94, loss: 2.294024\n",
      "Epoch 95, loss: 2.293940\n",
      "Epoch 96, loss: 2.293856\n",
      "Epoch 97, loss: 2.293770\n",
      "Epoch 98, loss: 2.293689\n",
      "Epoch 99, loss: 2.293601\n",
      "Epoch 100, loss: 2.293519\n",
      "Epoch 101, loss: 2.293432\n",
      "Epoch 102, loss: 2.293350\n",
      "Epoch 103, loss: 2.293263\n",
      "Epoch 104, loss: 2.293181\n",
      "Epoch 105, loss: 2.293096\n",
      "Epoch 106, loss: 2.293011\n",
      "Epoch 107, loss: 2.292925\n",
      "Epoch 108, loss: 2.292844\n",
      "Epoch 109, loss: 2.292760\n",
      "Epoch 110, loss: 2.292676\n",
      "Epoch 111, loss: 2.292591\n",
      "Epoch 112, loss: 2.292506\n",
      "Epoch 113, loss: 2.292424\n",
      "Epoch 114, loss: 2.292343\n",
      "Epoch 115, loss: 2.292257\n",
      "Epoch 116, loss: 2.292175\n",
      "Epoch 117, loss: 2.292089\n",
      "Epoch 118, loss: 2.292007\n",
      "Epoch 119, loss: 2.291925\n",
      "Epoch 120, loss: 2.291840\n",
      "Epoch 121, loss: 2.291758\n",
      "Epoch 122, loss: 2.291676\n",
      "Epoch 123, loss: 2.291592\n",
      "Epoch 124, loss: 2.291510\n",
      "Epoch 125, loss: 2.291427\n",
      "Epoch 126, loss: 2.291344\n",
      "Epoch 127, loss: 2.291261\n",
      "Epoch 128, loss: 2.291180\n",
      "Epoch 129, loss: 2.291097\n",
      "Epoch 130, loss: 2.291013\n",
      "Epoch 131, loss: 2.290931\n",
      "Epoch 132, loss: 2.290850\n",
      "Epoch 133, loss: 2.290767\n",
      "Epoch 134, loss: 2.290686\n",
      "Epoch 135, loss: 2.290604\n",
      "Epoch 136, loss: 2.290525\n",
      "Epoch 137, loss: 2.290439\n",
      "Epoch 138, loss: 2.290359\n",
      "Epoch 139, loss: 2.290276\n",
      "Epoch 140, loss: 2.290195\n",
      "Epoch 141, loss: 2.290114\n",
      "Epoch 142, loss: 2.290033\n",
      "Epoch 143, loss: 2.289951\n",
      "Epoch 144, loss: 2.289868\n",
      "Epoch 145, loss: 2.289788\n",
      "Epoch 146, loss: 2.289705\n",
      "Epoch 147, loss: 2.289625\n",
      "Epoch 148, loss: 2.289545\n",
      "Epoch 149, loss: 2.289463\n",
      "Epoch 150, loss: 2.289382\n",
      "Epoch 151, loss: 2.289302\n",
      "Epoch 152, loss: 2.289221\n",
      "Epoch 153, loss: 2.289140\n",
      "Epoch 154, loss: 2.289058\n",
      "Epoch 155, loss: 2.288978\n",
      "Epoch 156, loss: 2.288898\n",
      "Epoch 157, loss: 2.288816\n",
      "Epoch 158, loss: 2.288738\n",
      "Epoch 159, loss: 2.288656\n",
      "Epoch 160, loss: 2.288575\n",
      "Epoch 161, loss: 2.288496\n",
      "Epoch 162, loss: 2.288415\n",
      "Epoch 163, loss: 2.288336\n",
      "Epoch 164, loss: 2.288256\n",
      "Epoch 165, loss: 2.288177\n",
      "Epoch 166, loss: 2.288096\n",
      "Epoch 167, loss: 2.288018\n",
      "Epoch 168, loss: 2.287936\n",
      "Epoch 169, loss: 2.287857\n",
      "Epoch 170, loss: 2.287777\n",
      "Epoch 171, loss: 2.287697\n",
      "Epoch 172, loss: 2.287619\n",
      "Epoch 173, loss: 2.287538\n",
      "Epoch 174, loss: 2.287460\n",
      "Epoch 175, loss: 2.287380\n",
      "Epoch 176, loss: 2.287302\n",
      "Epoch 177, loss: 2.287221\n",
      "Epoch 178, loss: 2.287142\n",
      "Epoch 179, loss: 2.287062\n",
      "Epoch 180, loss: 2.286984\n",
      "Epoch 181, loss: 2.286906\n",
      "Epoch 182, loss: 2.286827\n",
      "Epoch 183, loss: 2.286750\n",
      "Epoch 184, loss: 2.286671\n",
      "Epoch 185, loss: 2.286589\n",
      "Epoch 186, loss: 2.286511\n",
      "Epoch 187, loss: 2.286434\n",
      "Epoch 188, loss: 2.286353\n",
      "Epoch 189, loss: 2.286276\n",
      "Epoch 190, loss: 2.286197\n",
      "Epoch 191, loss: 2.286120\n",
      "Epoch 192, loss: 2.286041\n",
      "Epoch 193, loss: 2.285962\n",
      "Epoch 194, loss: 2.285885\n",
      "Epoch 195, loss: 2.285806\n",
      "Epoch 196, loss: 2.285728\n",
      "Epoch 197, loss: 2.285653\n",
      "Epoch 198, loss: 2.285574\n",
      "Epoch 199, loss: 2.285495\n",
      "Accuracy = 0.170000\n",
      "\n",
      "Learning rate = 0.000100\n",
      "Reg strength = 0.000010\n",
      "Epoch 0, loss: 2.302504\n",
      "Epoch 1, loss: 2.302391\n",
      "Epoch 2, loss: 2.302277\n",
      "Epoch 3, loss: 2.302168\n",
      "Epoch 4, loss: 2.302056\n",
      "Epoch 5, loss: 2.301948\n",
      "Epoch 6, loss: 2.301837\n",
      "Epoch 7, loss: 2.301729\n",
      "Epoch 8, loss: 2.301623\n",
      "Epoch 9, loss: 2.301518\n",
      "Epoch 10, loss: 2.301411\n",
      "Epoch 11, loss: 2.301308\n",
      "Epoch 12, loss: 2.301201\n",
      "Epoch 13, loss: 2.301100\n",
      "Epoch 14, loss: 2.300996\n",
      "Epoch 15, loss: 2.300894\n",
      "Epoch 16, loss: 2.300792\n",
      "Epoch 17, loss: 2.300692\n",
      "Epoch 18, loss: 2.300589\n",
      "Epoch 19, loss: 2.300489\n",
      "Epoch 20, loss: 2.300393\n",
      "Epoch 21, loss: 2.300291\n",
      "Epoch 22, loss: 2.300192\n",
      "Epoch 23, loss: 2.300095\n",
      "Epoch 24, loss: 2.299996\n",
      "Epoch 25, loss: 2.299899\n",
      "Epoch 26, loss: 2.299801\n",
      "Epoch 27, loss: 2.299705\n",
      "Epoch 28, loss: 2.299609\n",
      "Epoch 29, loss: 2.299514\n",
      "Epoch 30, loss: 2.299417\n",
      "Epoch 31, loss: 2.299322\n",
      "Epoch 32, loss: 2.299228\n",
      "Epoch 33, loss: 2.299133\n",
      "Epoch 34, loss: 2.299039\n",
      "Epoch 35, loss: 2.298944\n",
      "Epoch 36, loss: 2.298851\n",
      "Epoch 37, loss: 2.298757\n",
      "Epoch 38, loss: 2.298666\n",
      "Epoch 39, loss: 2.298572\n",
      "Epoch 40, loss: 2.298478\n",
      "Epoch 41, loss: 2.298387\n",
      "Epoch 42, loss: 2.298295\n",
      "Epoch 43, loss: 2.298203\n",
      "Epoch 44, loss: 2.298110\n",
      "Epoch 45, loss: 2.298022\n",
      "Epoch 46, loss: 2.297929\n",
      "Epoch 47, loss: 2.297839\n",
      "Epoch 48, loss: 2.297748\n",
      "Epoch 49, loss: 2.297658\n",
      "Epoch 50, loss: 2.297567\n",
      "Epoch 51, loss: 2.297475\n",
      "Epoch 52, loss: 2.297384\n",
      "Epoch 53, loss: 2.297296\n",
      "Epoch 54, loss: 2.297206\n",
      "Epoch 55, loss: 2.297117\n",
      "Epoch 56, loss: 2.297028\n",
      "Epoch 57, loss: 2.296938\n",
      "Epoch 58, loss: 2.296851\n",
      "Epoch 59, loss: 2.296762\n",
      "Epoch 60, loss: 2.296673\n",
      "Epoch 61, loss: 2.296584\n",
      "Epoch 62, loss: 2.296497\n",
      "Epoch 63, loss: 2.296405\n",
      "Epoch 64, loss: 2.296317\n",
      "Epoch 65, loss: 2.296230\n",
      "Epoch 66, loss: 2.296140\n",
      "Epoch 67, loss: 2.296054\n",
      "Epoch 68, loss: 2.295968\n",
      "Epoch 69, loss: 2.295881\n",
      "Epoch 70, loss: 2.295791\n",
      "Epoch 71, loss: 2.295706\n",
      "Epoch 72, loss: 2.295620\n",
      "Epoch 73, loss: 2.295532\n",
      "Epoch 74, loss: 2.295445\n",
      "Epoch 75, loss: 2.295358\n",
      "Epoch 76, loss: 2.295273\n",
      "Epoch 77, loss: 2.295186\n",
      "Epoch 78, loss: 2.295101\n",
      "Epoch 79, loss: 2.295013\n",
      "Epoch 80, loss: 2.294929\n",
      "Epoch 81, loss: 2.294842\n",
      "Epoch 82, loss: 2.294754\n",
      "Epoch 83, loss: 2.294669\n",
      "Epoch 84, loss: 2.294584\n",
      "Epoch 85, loss: 2.294496\n",
      "Epoch 86, loss: 2.294413\n",
      "Epoch 87, loss: 2.294326\n",
      "Epoch 88, loss: 2.294240\n",
      "Epoch 89, loss: 2.294156\n",
      "Epoch 90, loss: 2.294071\n",
      "Epoch 91, loss: 2.293985\n",
      "Epoch 92, loss: 2.293904\n",
      "Epoch 93, loss: 2.293816\n",
      "Epoch 94, loss: 2.293729\n",
      "Epoch 95, loss: 2.293646\n",
      "Epoch 96, loss: 2.293561\n",
      "Epoch 97, loss: 2.293477\n",
      "Epoch 98, loss: 2.293393\n",
      "Epoch 99, loss: 2.293309\n",
      "Epoch 100, loss: 2.293225\n",
      "Epoch 101, loss: 2.293139\n",
      "Epoch 102, loss: 2.293056\n",
      "Epoch 103, loss: 2.292972\n",
      "Epoch 104, loss: 2.292890\n",
      "Epoch 105, loss: 2.292805\n",
      "Epoch 106, loss: 2.292719\n",
      "Epoch 107, loss: 2.292636\n",
      "Epoch 108, loss: 2.292552\n",
      "Epoch 109, loss: 2.292468\n",
      "Epoch 110, loss: 2.292386\n",
      "Epoch 111, loss: 2.292301\n",
      "Epoch 112, loss: 2.292220\n",
      "Epoch 113, loss: 2.292137\n",
      "Epoch 114, loss: 2.292052\n",
      "Epoch 115, loss: 2.291970\n",
      "Epoch 116, loss: 2.291887\n",
      "Epoch 117, loss: 2.291804\n",
      "Epoch 118, loss: 2.291722\n",
      "Epoch 119, loss: 2.291640\n",
      "Epoch 120, loss: 2.291556\n",
      "Epoch 121, loss: 2.291473\n",
      "Epoch 122, loss: 2.291390\n",
      "Epoch 123, loss: 2.291309\n",
      "Epoch 124, loss: 2.291226\n",
      "Epoch 125, loss: 2.291143\n",
      "Epoch 126, loss: 2.291061\n",
      "Epoch 127, loss: 2.290979\n",
      "Epoch 128, loss: 2.290897\n",
      "Epoch 129, loss: 2.290813\n",
      "Epoch 130, loss: 2.290732\n",
      "Epoch 131, loss: 2.290650\n",
      "Epoch 132, loss: 2.290569\n",
      "Epoch 133, loss: 2.290487\n",
      "Epoch 134, loss: 2.290405\n",
      "Epoch 135, loss: 2.290322\n",
      "Epoch 136, loss: 2.290243\n",
      "Epoch 137, loss: 2.290161\n",
      "Epoch 138, loss: 2.290080\n",
      "Epoch 139, loss: 2.289997\n",
      "Epoch 140, loss: 2.289914\n",
      "Epoch 141, loss: 2.289835\n",
      "Epoch 142, loss: 2.289756\n",
      "Epoch 143, loss: 2.289673\n",
      "Epoch 144, loss: 2.289593\n",
      "Epoch 145, loss: 2.289509\n",
      "Epoch 146, loss: 2.289430\n",
      "Epoch 147, loss: 2.289350\n",
      "Epoch 148, loss: 2.289270\n",
      "Epoch 149, loss: 2.289188\n",
      "Epoch 150, loss: 2.289107\n",
      "Epoch 151, loss: 2.289025\n",
      "Epoch 152, loss: 2.288946\n",
      "Epoch 153, loss: 2.288865\n",
      "Epoch 154, loss: 2.288785\n",
      "Epoch 155, loss: 2.288704\n",
      "Epoch 156, loss: 2.288625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157, loss: 2.288545\n",
      "Epoch 158, loss: 2.288466\n",
      "Epoch 159, loss: 2.288384\n",
      "Epoch 160, loss: 2.288305\n",
      "Epoch 161, loss: 2.288224\n",
      "Epoch 162, loss: 2.288143\n",
      "Epoch 163, loss: 2.288066\n",
      "Epoch 164, loss: 2.287985\n",
      "Epoch 165, loss: 2.287905\n",
      "Epoch 166, loss: 2.287826\n",
      "Epoch 167, loss: 2.287747\n",
      "Epoch 168, loss: 2.287668\n",
      "Epoch 169, loss: 2.287588\n",
      "Epoch 170, loss: 2.287508\n",
      "Epoch 171, loss: 2.287430\n",
      "Epoch 172, loss: 2.287349\n",
      "Epoch 173, loss: 2.287273\n",
      "Epoch 174, loss: 2.287195\n",
      "Epoch 175, loss: 2.287113\n",
      "Epoch 176, loss: 2.287033\n",
      "Epoch 177, loss: 2.286955\n",
      "Epoch 178, loss: 2.286877\n",
      "Epoch 179, loss: 2.286798\n",
      "Epoch 180, loss: 2.286718\n",
      "Epoch 181, loss: 2.286640\n",
      "Epoch 182, loss: 2.286562\n",
      "Epoch 183, loss: 2.286485\n",
      "Epoch 184, loss: 2.286405\n",
      "Epoch 185, loss: 2.286328\n",
      "Epoch 186, loss: 2.286247\n",
      "Epoch 187, loss: 2.286169\n",
      "Epoch 188, loss: 2.286095\n",
      "Epoch 189, loss: 2.286013\n",
      "Epoch 190, loss: 2.285935\n",
      "Epoch 191, loss: 2.285857\n",
      "Epoch 192, loss: 2.285780\n",
      "Epoch 193, loss: 2.285704\n",
      "Epoch 194, loss: 2.285625\n",
      "Epoch 195, loss: 2.285547\n",
      "Epoch 196, loss: 2.285468\n",
      "Epoch 197, loss: 2.285392\n",
      "Epoch 198, loss: 2.285314\n",
      "Epoch 199, loss: 2.285235\n",
      "Accuracy = 0.171000\n",
      "\n",
      "Learning rate = 0.000100\n",
      "Reg strength = 0.000001\n",
      "Epoch 0, loss: 2.302287\n",
      "Epoch 1, loss: 2.302179\n",
      "Epoch 2, loss: 2.302070\n",
      "Epoch 3, loss: 2.301965\n",
      "Epoch 4, loss: 2.301859\n",
      "Epoch 5, loss: 2.301752\n",
      "Epoch 6, loss: 2.301652\n",
      "Epoch 7, loss: 2.301545\n",
      "Epoch 8, loss: 2.301443\n",
      "Epoch 9, loss: 2.301338\n",
      "Epoch 10, loss: 2.301238\n",
      "Epoch 11, loss: 2.301135\n",
      "Epoch 12, loss: 2.301037\n",
      "Epoch 13, loss: 2.300934\n",
      "Epoch 14, loss: 2.300835\n",
      "Epoch 15, loss: 2.300735\n",
      "Epoch 16, loss: 2.300635\n",
      "Epoch 17, loss: 2.300540\n",
      "Epoch 18, loss: 2.300440\n",
      "Epoch 19, loss: 2.300344\n",
      "Epoch 20, loss: 2.300245\n",
      "Epoch 21, loss: 2.300149\n",
      "Epoch 22, loss: 2.300053\n",
      "Epoch 23, loss: 2.299956\n",
      "Epoch 24, loss: 2.299859\n",
      "Epoch 25, loss: 2.299765\n",
      "Epoch 26, loss: 2.299670\n",
      "Epoch 27, loss: 2.299576\n",
      "Epoch 28, loss: 2.299478\n",
      "Epoch 29, loss: 2.299385\n",
      "Epoch 30, loss: 2.299291\n",
      "Epoch 31, loss: 2.299197\n",
      "Epoch 32, loss: 2.299104\n",
      "Epoch 33, loss: 2.299012\n",
      "Epoch 34, loss: 2.298919\n",
      "Epoch 35, loss: 2.298824\n",
      "Epoch 36, loss: 2.298734\n",
      "Epoch 37, loss: 2.298644\n",
      "Epoch 38, loss: 2.298549\n",
      "Epoch 39, loss: 2.298458\n",
      "Epoch 40, loss: 2.298367\n",
      "Epoch 41, loss: 2.298275\n",
      "Epoch 42, loss: 2.298184\n",
      "Epoch 43, loss: 2.298091\n",
      "Epoch 44, loss: 2.297999\n",
      "Epoch 45, loss: 2.297912\n",
      "Epoch 46, loss: 2.297820\n",
      "Epoch 47, loss: 2.297729\n",
      "Epoch 48, loss: 2.297640\n",
      "Epoch 49, loss: 2.297550\n",
      "Epoch 50, loss: 2.297463\n",
      "Epoch 51, loss: 2.297371\n",
      "Epoch 52, loss: 2.297280\n",
      "Epoch 53, loss: 2.297193\n",
      "Epoch 54, loss: 2.297103\n",
      "Epoch 55, loss: 2.297014\n",
      "Epoch 56, loss: 2.296927\n",
      "Epoch 57, loss: 2.296837\n",
      "Epoch 58, loss: 2.296750\n",
      "Epoch 59, loss: 2.296662\n",
      "Epoch 60, loss: 2.296573\n",
      "Epoch 61, loss: 2.296482\n",
      "Epoch 62, loss: 2.296396\n",
      "Epoch 63, loss: 2.296307\n",
      "Epoch 64, loss: 2.296221\n",
      "Epoch 65, loss: 2.296132\n",
      "Epoch 66, loss: 2.296045\n",
      "Epoch 67, loss: 2.295957\n",
      "Epoch 68, loss: 2.295871\n",
      "Epoch 69, loss: 2.295785\n",
      "Epoch 70, loss: 2.295698\n",
      "Epoch 71, loss: 2.295611\n",
      "Epoch 72, loss: 2.295524\n",
      "Epoch 73, loss: 2.295436\n",
      "Epoch 74, loss: 2.295348\n",
      "Epoch 75, loss: 2.295264\n",
      "Epoch 76, loss: 2.295178\n",
      "Epoch 77, loss: 2.295092\n",
      "Epoch 78, loss: 2.295005\n",
      "Epoch 79, loss: 2.294920\n",
      "Epoch 80, loss: 2.294834\n",
      "Epoch 81, loss: 2.294748\n",
      "Epoch 82, loss: 2.294661\n",
      "Epoch 83, loss: 2.294576\n",
      "Epoch 84, loss: 2.294491\n",
      "Epoch 85, loss: 2.294404\n",
      "Epoch 86, loss: 2.294319\n",
      "Epoch 87, loss: 2.294235\n",
      "Epoch 88, loss: 2.294150\n",
      "Epoch 89, loss: 2.294066\n",
      "Epoch 90, loss: 2.293981\n",
      "Epoch 91, loss: 2.293893\n",
      "Epoch 92, loss: 2.293810\n",
      "Epoch 93, loss: 2.293724\n",
      "Epoch 94, loss: 2.293639\n",
      "Epoch 95, loss: 2.293555\n",
      "Epoch 96, loss: 2.293471\n",
      "Epoch 97, loss: 2.293386\n",
      "Epoch 98, loss: 2.293302\n",
      "Epoch 99, loss: 2.293218\n",
      "Epoch 100, loss: 2.293134\n",
      "Epoch 101, loss: 2.293050\n",
      "Epoch 102, loss: 2.292965\n",
      "Epoch 103, loss: 2.292883\n",
      "Epoch 104, loss: 2.292797\n",
      "Epoch 105, loss: 2.292713\n",
      "Epoch 106, loss: 2.292630\n",
      "Epoch 107, loss: 2.292547\n",
      "Epoch 108, loss: 2.292463\n",
      "Epoch 109, loss: 2.292381\n",
      "Epoch 110, loss: 2.292295\n",
      "Epoch 111, loss: 2.292212\n",
      "Epoch 112, loss: 2.292131\n",
      "Epoch 113, loss: 2.292046\n",
      "Epoch 114, loss: 2.291963\n",
      "Epoch 115, loss: 2.291881\n",
      "Epoch 116, loss: 2.291798\n",
      "Epoch 117, loss: 2.291716\n",
      "Epoch 118, loss: 2.291631\n",
      "Epoch 119, loss: 2.291550\n",
      "Epoch 120, loss: 2.291466\n",
      "Epoch 121, loss: 2.291384\n",
      "Epoch 122, loss: 2.291303\n",
      "Epoch 123, loss: 2.291219\n",
      "Epoch 124, loss: 2.291136\n",
      "Epoch 125, loss: 2.291055\n",
      "Epoch 126, loss: 2.290971\n",
      "Epoch 127, loss: 2.290889\n",
      "Epoch 128, loss: 2.290808\n",
      "Epoch 129, loss: 2.290726\n",
      "Epoch 130, loss: 2.290646\n",
      "Epoch 131, loss: 2.290562\n",
      "Epoch 132, loss: 2.290480\n",
      "Epoch 133, loss: 2.290397\n",
      "Epoch 134, loss: 2.290317\n",
      "Epoch 135, loss: 2.290236\n",
      "Epoch 136, loss: 2.290152\n",
      "Epoch 137, loss: 2.290072\n",
      "Epoch 138, loss: 2.289992\n",
      "Epoch 139, loss: 2.289911\n",
      "Epoch 140, loss: 2.289828\n",
      "Epoch 141, loss: 2.289749\n",
      "Epoch 142, loss: 2.289669\n",
      "Epoch 143, loss: 2.289586\n",
      "Epoch 144, loss: 2.289504\n",
      "Epoch 145, loss: 2.289423\n",
      "Epoch 146, loss: 2.289342\n",
      "Epoch 147, loss: 2.289261\n",
      "Epoch 148, loss: 2.289182\n",
      "Epoch 149, loss: 2.289099\n",
      "Epoch 150, loss: 2.289019\n",
      "Epoch 151, loss: 2.288939\n",
      "Epoch 152, loss: 2.288859\n",
      "Epoch 153, loss: 2.288778\n",
      "Epoch 154, loss: 2.288698\n",
      "Epoch 155, loss: 2.288619\n",
      "Epoch 156, loss: 2.288536\n",
      "Epoch 157, loss: 2.288458\n",
      "Epoch 158, loss: 2.288378\n",
      "Epoch 159, loss: 2.288299\n",
      "Epoch 160, loss: 2.288216\n",
      "Epoch 161, loss: 2.288138\n",
      "Epoch 162, loss: 2.288058\n",
      "Epoch 163, loss: 2.287978\n",
      "Epoch 164, loss: 2.287898\n",
      "Epoch 165, loss: 2.287819\n",
      "Epoch 166, loss: 2.287740\n",
      "Epoch 167, loss: 2.287659\n",
      "Epoch 168, loss: 2.287580\n",
      "Epoch 169, loss: 2.287501\n",
      "Epoch 170, loss: 2.287421\n",
      "Epoch 171, loss: 2.287341\n",
      "Epoch 172, loss: 2.287266\n",
      "Epoch 173, loss: 2.287183\n",
      "Epoch 174, loss: 2.287105\n",
      "Epoch 175, loss: 2.287027\n",
      "Epoch 176, loss: 2.286949\n",
      "Epoch 177, loss: 2.286868\n",
      "Epoch 178, loss: 2.286791\n",
      "Epoch 179, loss: 2.286712\n",
      "Epoch 180, loss: 2.286632\n",
      "Epoch 181, loss: 2.286554\n",
      "Epoch 182, loss: 2.286476\n",
      "Epoch 183, loss: 2.286398\n",
      "Epoch 184, loss: 2.286318\n",
      "Epoch 185, loss: 2.286240\n",
      "Epoch 186, loss: 2.286164\n",
      "Epoch 187, loss: 2.286082\n",
      "Epoch 188, loss: 2.286005\n",
      "Epoch 189, loss: 2.285927\n",
      "Epoch 190, loss: 2.285849\n",
      "Epoch 191, loss: 2.285772\n",
      "Epoch 192, loss: 2.285694\n",
      "Epoch 193, loss: 2.285616\n",
      "Epoch 194, loss: 2.285539\n",
      "Epoch 195, loss: 2.285460\n",
      "Epoch 196, loss: 2.285383\n",
      "Epoch 197, loss: 2.285305\n",
      "Epoch 198, loss: 2.285227\n",
      "Epoch 199, loss: 2.285149\n",
      "Accuracy = 0.169000\n",
      "\n",
      "Learning rate = 0.000010\n",
      "Reg strength = 0.000100\n",
      "Epoch 0, loss: 2.302940\n",
      "Epoch 1, loss: 2.302929\n",
      "Epoch 2, loss: 2.302917\n",
      "Epoch 3, loss: 2.302906\n",
      "Epoch 4, loss: 2.302894\n",
      "Epoch 5, loss: 2.302883\n",
      "Epoch 6, loss: 2.302871\n",
      "Epoch 7, loss: 2.302860\n",
      "Epoch 8, loss: 2.302848\n",
      "Epoch 9, loss: 2.302837\n",
      "Epoch 10, loss: 2.302825\n",
      "Epoch 11, loss: 2.302814\n",
      "Epoch 12, loss: 2.302803\n",
      "Epoch 13, loss: 2.302791\n",
      "Epoch 14, loss: 2.302780\n",
      "Epoch 15, loss: 2.302768\n",
      "Epoch 16, loss: 2.302757\n",
      "Epoch 17, loss: 2.302746\n",
      "Epoch 18, loss: 2.302734\n",
      "Epoch 19, loss: 2.302723\n",
      "Epoch 20, loss: 2.302711\n",
      "Epoch 21, loss: 2.302700\n",
      "Epoch 22, loss: 2.302689\n",
      "Epoch 23, loss: 2.302678\n",
      "Epoch 24, loss: 2.302666\n",
      "Epoch 25, loss: 2.302655\n",
      "Epoch 26, loss: 2.302644\n",
      "Epoch 27, loss: 2.302632\n",
      "Epoch 28, loss: 2.302621\n",
      "Epoch 29, loss: 2.302610\n",
      "Epoch 30, loss: 2.302599\n",
      "Epoch 31, loss: 2.302587\n",
      "Epoch 32, loss: 2.302576\n",
      "Epoch 33, loss: 2.302565\n",
      "Epoch 34, loss: 2.302554\n",
      "Epoch 35, loss: 2.302542\n",
      "Epoch 36, loss: 2.302531\n",
      "Epoch 37, loss: 2.302520\n",
      "Epoch 38, loss: 2.302509\n",
      "Epoch 39, loss: 2.302498\n",
      "Epoch 40, loss: 2.302487\n",
      "Epoch 41, loss: 2.302476\n",
      "Epoch 42, loss: 2.302464\n",
      "Epoch 43, loss: 2.302453\n",
      "Epoch 44, loss: 2.302442\n",
      "Epoch 45, loss: 2.302431\n",
      "Epoch 46, loss: 2.302420\n",
      "Epoch 47, loss: 2.302409\n",
      "Epoch 48, loss: 2.302398\n",
      "Epoch 49, loss: 2.302387\n",
      "Epoch 50, loss: 2.302376\n",
      "Epoch 51, loss: 2.302364\n",
      "Epoch 52, loss: 2.302354\n",
      "Epoch 53, loss: 2.302342\n",
      "Epoch 54, loss: 2.302331\n",
      "Epoch 55, loss: 2.302320\n",
      "Epoch 56, loss: 2.302310\n",
      "Epoch 57, loss: 2.302298\n",
      "Epoch 58, loss: 2.302288\n",
      "Epoch 59, loss: 2.302276\n",
      "Epoch 60, loss: 2.302265\n",
      "Epoch 61, loss: 2.302255\n",
      "Epoch 62, loss: 2.302244\n",
      "Epoch 63, loss: 2.302233\n",
      "Epoch 64, loss: 2.302222\n",
      "Epoch 65, loss: 2.302211\n",
      "Epoch 66, loss: 2.302200\n",
      "Epoch 67, loss: 2.302189\n",
      "Epoch 68, loss: 2.302178\n",
      "Epoch 69, loss: 2.302167\n",
      "Epoch 70, loss: 2.302156\n",
      "Epoch 71, loss: 2.302145\n",
      "Epoch 72, loss: 2.302134\n",
      "Epoch 73, loss: 2.302124\n",
      "Epoch 74, loss: 2.302113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, loss: 2.302102\n",
      "Epoch 76, loss: 2.302091\n",
      "Epoch 77, loss: 2.302080\n",
      "Epoch 78, loss: 2.302069\n",
      "Epoch 79, loss: 2.302058\n",
      "Epoch 80, loss: 2.302048\n",
      "Epoch 81, loss: 2.302037\n",
      "Epoch 82, loss: 2.302026\n",
      "Epoch 83, loss: 2.302015\n",
      "Epoch 84, loss: 2.302005\n",
      "Epoch 85, loss: 2.301994\n",
      "Epoch 86, loss: 2.301983\n",
      "Epoch 87, loss: 2.301972\n",
      "Epoch 88, loss: 2.301962\n",
      "Epoch 89, loss: 2.301951\n",
      "Epoch 90, loss: 2.301940\n",
      "Epoch 91, loss: 2.301929\n",
      "Epoch 92, loss: 2.301919\n",
      "Epoch 93, loss: 2.301908\n",
      "Epoch 94, loss: 2.301897\n",
      "Epoch 95, loss: 2.301887\n",
      "Epoch 96, loss: 2.301876\n",
      "Epoch 97, loss: 2.301865\n",
      "Epoch 98, loss: 2.301854\n",
      "Epoch 99, loss: 2.301844\n",
      "Epoch 100, loss: 2.301833\n",
      "Epoch 101, loss: 2.301822\n",
      "Epoch 102, loss: 2.301812\n",
      "Epoch 103, loss: 2.301801\n",
      "Epoch 104, loss: 2.301791\n",
      "Epoch 105, loss: 2.301780\n",
      "Epoch 106, loss: 2.301769\n",
      "Epoch 107, loss: 2.301759\n",
      "Epoch 108, loss: 2.301748\n",
      "Epoch 109, loss: 2.301738\n",
      "Epoch 110, loss: 2.301727\n",
      "Epoch 111, loss: 2.301717\n",
      "Epoch 112, loss: 2.301706\n",
      "Epoch 113, loss: 2.301695\n",
      "Epoch 114, loss: 2.301685\n",
      "Epoch 115, loss: 2.301674\n",
      "Epoch 116, loss: 2.301664\n",
      "Epoch 117, loss: 2.301653\n",
      "Epoch 118, loss: 2.301643\n",
      "Epoch 119, loss: 2.301632\n",
      "Epoch 120, loss: 2.301622\n",
      "Epoch 121, loss: 2.301611\n",
      "Epoch 122, loss: 2.301600\n",
      "Epoch 123, loss: 2.301590\n",
      "Epoch 124, loss: 2.301579\n",
      "Epoch 125, loss: 2.301569\n",
      "Epoch 126, loss: 2.301559\n",
      "Epoch 127, loss: 2.301548\n",
      "Epoch 128, loss: 2.301538\n",
      "Epoch 129, loss: 2.301527\n",
      "Epoch 130, loss: 2.301517\n",
      "Epoch 131, loss: 2.301506\n",
      "Epoch 132, loss: 2.301496\n",
      "Epoch 133, loss: 2.301486\n",
      "Epoch 134, loss: 2.301475\n",
      "Epoch 135, loss: 2.301465\n",
      "Epoch 136, loss: 2.301454\n",
      "Epoch 137, loss: 2.301444\n",
      "Epoch 138, loss: 2.301433\n",
      "Epoch 139, loss: 2.301423\n",
      "Epoch 140, loss: 2.301413\n",
      "Epoch 141, loss: 2.301402\n",
      "Epoch 142, loss: 2.301392\n",
      "Epoch 143, loss: 2.301382\n",
      "Epoch 144, loss: 2.301371\n",
      "Epoch 145, loss: 2.301361\n",
      "Epoch 146, loss: 2.301350\n",
      "Epoch 147, loss: 2.301340\n",
      "Epoch 148, loss: 2.301330\n",
      "Epoch 149, loss: 2.301320\n",
      "Epoch 150, loss: 2.301309\n",
      "Epoch 151, loss: 2.301299\n",
      "Epoch 152, loss: 2.301288\n",
      "Epoch 153, loss: 2.301278\n",
      "Epoch 154, loss: 2.301268\n",
      "Epoch 155, loss: 2.301258\n",
      "Epoch 156, loss: 2.301247\n",
      "Epoch 157, loss: 2.301237\n",
      "Epoch 158, loss: 2.301227\n",
      "Epoch 159, loss: 2.301217\n",
      "Epoch 160, loss: 2.301206\n",
      "Epoch 161, loss: 2.301196\n",
      "Epoch 162, loss: 2.301185\n",
      "Epoch 163, loss: 2.301175\n",
      "Epoch 164, loss: 2.301165\n",
      "Epoch 165, loss: 2.301155\n",
      "Epoch 166, loss: 2.301145\n",
      "Epoch 167, loss: 2.301135\n",
      "Epoch 168, loss: 2.301124\n",
      "Epoch 169, loss: 2.301114\n",
      "Epoch 170, loss: 2.301104\n",
      "Epoch 171, loss: 2.301094\n",
      "Epoch 172, loss: 2.301084\n",
      "Epoch 173, loss: 2.301073\n",
      "Epoch 174, loss: 2.301063\n",
      "Epoch 175, loss: 2.301053\n",
      "Epoch 176, loss: 2.301043\n",
      "Epoch 177, loss: 2.301033\n",
      "Epoch 178, loss: 2.301023\n",
      "Epoch 179, loss: 2.301012\n",
      "Epoch 180, loss: 2.301002\n",
      "Epoch 181, loss: 2.300992\n",
      "Epoch 182, loss: 2.300982\n",
      "Epoch 183, loss: 2.300972\n",
      "Epoch 184, loss: 2.300962\n",
      "Epoch 185, loss: 2.300951\n",
      "Epoch 186, loss: 2.300941\n",
      "Epoch 187, loss: 2.300931\n",
      "Epoch 188, loss: 2.300922\n",
      "Epoch 189, loss: 2.300911\n",
      "Epoch 190, loss: 2.300901\n",
      "Epoch 191, loss: 2.300891\n",
      "Epoch 192, loss: 2.300881\n",
      "Epoch 193, loss: 2.300871\n",
      "Epoch 194, loss: 2.300861\n",
      "Epoch 195, loss: 2.300850\n",
      "Epoch 196, loss: 2.300840\n",
      "Epoch 197, loss: 2.300830\n",
      "Epoch 198, loss: 2.300820\n",
      "Epoch 199, loss: 2.300810\n",
      "Accuracy = 0.120000\n",
      "\n",
      "Learning rate = 0.000010\n",
      "Reg strength = 0.000010\n",
      "Epoch 0, loss: 2.302856\n",
      "Epoch 1, loss: 2.302845\n",
      "Epoch 2, loss: 2.302833\n",
      "Epoch 3, loss: 2.302821\n",
      "Epoch 4, loss: 2.302810\n",
      "Epoch 5, loss: 2.302798\n",
      "Epoch 6, loss: 2.302786\n",
      "Epoch 7, loss: 2.302775\n",
      "Epoch 8, loss: 2.302763\n",
      "Epoch 9, loss: 2.302751\n",
      "Epoch 10, loss: 2.302740\n",
      "Epoch 11, loss: 2.302728\n",
      "Epoch 12, loss: 2.302716\n",
      "Epoch 13, loss: 2.302705\n",
      "Epoch 14, loss: 2.302693\n",
      "Epoch 15, loss: 2.302682\n",
      "Epoch 16, loss: 2.302670\n",
      "Epoch 17, loss: 2.302659\n",
      "Epoch 18, loss: 2.302647\n",
      "Epoch 19, loss: 2.302635\n",
      "Epoch 20, loss: 2.302624\n",
      "Epoch 21, loss: 2.302612\n",
      "Epoch 22, loss: 2.302601\n",
      "Epoch 23, loss: 2.302589\n",
      "Epoch 24, loss: 2.302578\n",
      "Epoch 25, loss: 2.302566\n",
      "Epoch 26, loss: 2.302555\n",
      "Epoch 27, loss: 2.302543\n",
      "Epoch 28, loss: 2.302532\n",
      "Epoch 29, loss: 2.302520\n",
      "Epoch 30, loss: 2.302509\n",
      "Epoch 31, loss: 2.302498\n",
      "Epoch 32, loss: 2.302486\n",
      "Epoch 33, loss: 2.302475\n",
      "Epoch 34, loss: 2.302463\n",
      "Epoch 35, loss: 2.302452\n",
      "Epoch 36, loss: 2.302441\n",
      "Epoch 37, loss: 2.302429\n",
      "Epoch 38, loss: 2.302418\n",
      "Epoch 39, loss: 2.302407\n",
      "Epoch 40, loss: 2.302395\n",
      "Epoch 41, loss: 2.302384\n",
      "Epoch 42, loss: 2.302372\n",
      "Epoch 43, loss: 2.302361\n",
      "Epoch 44, loss: 2.302350\n",
      "Epoch 45, loss: 2.302339\n",
      "Epoch 46, loss: 2.302327\n",
      "Epoch 47, loss: 2.302316\n",
      "Epoch 48, loss: 2.302305\n",
      "Epoch 49, loss: 2.302294\n",
      "Epoch 50, loss: 2.302282\n",
      "Epoch 51, loss: 2.302271\n",
      "Epoch 52, loss: 2.302260\n",
      "Epoch 53, loss: 2.302249\n",
      "Epoch 54, loss: 2.302238\n",
      "Epoch 55, loss: 2.302226\n",
      "Epoch 56, loss: 2.302215\n",
      "Epoch 57, loss: 2.302204\n",
      "Epoch 58, loss: 2.302193\n",
      "Epoch 59, loss: 2.302182\n",
      "Epoch 60, loss: 2.302170\n",
      "Epoch 61, loss: 2.302160\n",
      "Epoch 62, loss: 2.302148\n",
      "Epoch 63, loss: 2.302137\n",
      "Epoch 64, loss: 2.302126\n",
      "Epoch 65, loss: 2.302115\n",
      "Epoch 66, loss: 2.302104\n",
      "Epoch 67, loss: 2.302093\n",
      "Epoch 68, loss: 2.302082\n",
      "Epoch 69, loss: 2.302071\n",
      "Epoch 70, loss: 2.302060\n",
      "Epoch 71, loss: 2.302049\n",
      "Epoch 72, loss: 2.302037\n",
      "Epoch 73, loss: 2.302027\n",
      "Epoch 74, loss: 2.302016\n",
      "Epoch 75, loss: 2.302004\n",
      "Epoch 76, loss: 2.301994\n",
      "Epoch 77, loss: 2.301983\n",
      "Epoch 78, loss: 2.301972\n",
      "Epoch 79, loss: 2.301961\n",
      "Epoch 80, loss: 2.301950\n",
      "Epoch 81, loss: 2.301939\n",
      "Epoch 82, loss: 2.301928\n",
      "Epoch 83, loss: 2.301917\n",
      "Epoch 84, loss: 2.301906\n",
      "Epoch 85, loss: 2.301895\n",
      "Epoch 86, loss: 2.301884\n",
      "Epoch 87, loss: 2.301873\n",
      "Epoch 88, loss: 2.301862\n",
      "Epoch 89, loss: 2.301851\n",
      "Epoch 90, loss: 2.301840\n",
      "Epoch 91, loss: 2.301830\n",
      "Epoch 92, loss: 2.301819\n",
      "Epoch 93, loss: 2.301808\n",
      "Epoch 94, loss: 2.301797\n",
      "Epoch 95, loss: 2.301786\n",
      "Epoch 96, loss: 2.301775\n",
      "Epoch 97, loss: 2.301765\n",
      "Epoch 98, loss: 2.301754\n",
      "Epoch 99, loss: 2.301743\n",
      "Epoch 100, loss: 2.301732\n",
      "Epoch 101, loss: 2.301721\n",
      "Epoch 102, loss: 2.301711\n",
      "Epoch 103, loss: 2.301700\n",
      "Epoch 104, loss: 2.301689\n",
      "Epoch 105, loss: 2.301678\n",
      "Epoch 106, loss: 2.301668\n",
      "Epoch 107, loss: 2.301657\n",
      "Epoch 108, loss: 2.301646\n",
      "Epoch 109, loss: 2.301635\n",
      "Epoch 110, loss: 2.301625\n",
      "Epoch 111, loss: 2.301614\n",
      "Epoch 112, loss: 2.301603\n",
      "Epoch 113, loss: 2.301592\n",
      "Epoch 114, loss: 2.301582\n",
      "Epoch 115, loss: 2.301571\n",
      "Epoch 116, loss: 2.301561\n",
      "Epoch 117, loss: 2.301550\n",
      "Epoch 118, loss: 2.301539\n",
      "Epoch 119, loss: 2.301528\n",
      "Epoch 120, loss: 2.301518\n",
      "Epoch 121, loss: 2.301507\n",
      "Epoch 122, loss: 2.301497\n",
      "Epoch 123, loss: 2.301486\n",
      "Epoch 124, loss: 2.301475\n",
      "Epoch 125, loss: 2.301465\n",
      "Epoch 126, loss: 2.301454\n",
      "Epoch 127, loss: 2.301443\n",
      "Epoch 128, loss: 2.301433\n",
      "Epoch 129, loss: 2.301422\n",
      "Epoch 130, loss: 2.301412\n",
      "Epoch 131, loss: 2.301401\n",
      "Epoch 132, loss: 2.301391\n",
      "Epoch 133, loss: 2.301380\n",
      "Epoch 134, loss: 2.301370\n",
      "Epoch 135, loss: 2.301359\n",
      "Epoch 136, loss: 2.301348\n",
      "Epoch 137, loss: 2.301338\n",
      "Epoch 138, loss: 2.301327\n",
      "Epoch 139, loss: 2.301317\n",
      "Epoch 140, loss: 2.301307\n",
      "Epoch 141, loss: 2.301296\n",
      "Epoch 142, loss: 2.301286\n",
      "Epoch 143, loss: 2.301275\n",
      "Epoch 144, loss: 2.301264\n",
      "Epoch 145, loss: 2.301254\n",
      "Epoch 146, loss: 2.301244\n",
      "Epoch 147, loss: 2.301233\n",
      "Epoch 148, loss: 2.301223\n",
      "Epoch 149, loss: 2.301212\n",
      "Epoch 150, loss: 2.301202\n",
      "Epoch 151, loss: 2.301191\n",
      "Epoch 152, loss: 2.301181\n",
      "Epoch 153, loss: 2.301170\n",
      "Epoch 154, loss: 2.301160\n",
      "Epoch 155, loss: 2.301150\n",
      "Epoch 156, loss: 2.301139\n",
      "Epoch 157, loss: 2.301129\n",
      "Epoch 158, loss: 2.301118\n",
      "Epoch 159, loss: 2.301108\n",
      "Epoch 160, loss: 2.301098\n",
      "Epoch 161, loss: 2.301087\n",
      "Epoch 162, loss: 2.301077\n",
      "Epoch 163, loss: 2.301067\n",
      "Epoch 164, loss: 2.301057\n",
      "Epoch 165, loss: 2.301046\n",
      "Epoch 166, loss: 2.301035\n",
      "Epoch 167, loss: 2.301025\n",
      "Epoch 168, loss: 2.301015\n",
      "Epoch 169, loss: 2.301004\n",
      "Epoch 170, loss: 2.300994\n",
      "Epoch 171, loss: 2.300984\n",
      "Epoch 172, loss: 2.300974\n",
      "Epoch 173, loss: 2.300963\n",
      "Epoch 174, loss: 2.300953\n",
      "Epoch 175, loss: 2.300943\n",
      "Epoch 176, loss: 2.300933\n",
      "Epoch 177, loss: 2.300923\n",
      "Epoch 178, loss: 2.300912\n",
      "Epoch 179, loss: 2.300902\n",
      "Epoch 180, loss: 2.300892\n",
      "Epoch 181, loss: 2.300881\n",
      "Epoch 182, loss: 2.300871\n",
      "Epoch 183, loss: 2.300861\n",
      "Epoch 184, loss: 2.300851\n",
      "Epoch 185, loss: 2.300840\n",
      "Epoch 186, loss: 2.300830\n",
      "Epoch 187, loss: 2.300820\n",
      "Epoch 188, loss: 2.300810\n",
      "Epoch 189, loss: 2.300800\n",
      "Epoch 190, loss: 2.300789\n",
      "Epoch 191, loss: 2.300779\n",
      "Epoch 192, loss: 2.300769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193, loss: 2.300759\n",
      "Epoch 194, loss: 2.300749\n",
      "Epoch 195, loss: 2.300738\n",
      "Epoch 196, loss: 2.300728\n",
      "Epoch 197, loss: 2.300718\n",
      "Epoch 198, loss: 2.300708\n",
      "Epoch 199, loss: 2.300698\n",
      "Accuracy = 0.128000\n",
      "\n",
      "Learning rate = 0.000010\n",
      "Reg strength = 0.000001\n",
      "Epoch 0, loss: 2.302753\n",
      "Epoch 1, loss: 2.302741\n",
      "Epoch 2, loss: 2.302730\n",
      "Epoch 3, loss: 2.302719\n",
      "Epoch 4, loss: 2.302708\n",
      "Epoch 5, loss: 2.302696\n",
      "Epoch 6, loss: 2.302685\n",
      "Epoch 7, loss: 2.302674\n",
      "Epoch 8, loss: 2.302663\n",
      "Epoch 9, loss: 2.302651\n",
      "Epoch 10, loss: 2.302640\n",
      "Epoch 11, loss: 2.302629\n",
      "Epoch 12, loss: 2.302617\n",
      "Epoch 13, loss: 2.302606\n",
      "Epoch 14, loss: 2.302595\n",
      "Epoch 15, loss: 2.302584\n",
      "Epoch 16, loss: 2.302573\n",
      "Epoch 17, loss: 2.302561\n",
      "Epoch 18, loss: 2.302550\n",
      "Epoch 19, loss: 2.302539\n",
      "Epoch 20, loss: 2.302528\n",
      "Epoch 21, loss: 2.302517\n",
      "Epoch 22, loss: 2.302506\n",
      "Epoch 23, loss: 2.302495\n",
      "Epoch 24, loss: 2.302484\n",
      "Epoch 25, loss: 2.302473\n",
      "Epoch 26, loss: 2.302461\n",
      "Epoch 27, loss: 2.302450\n",
      "Epoch 28, loss: 2.302439\n",
      "Epoch 29, loss: 2.302428\n",
      "Epoch 30, loss: 2.302417\n",
      "Epoch 31, loss: 2.302406\n",
      "Epoch 32, loss: 2.302395\n",
      "Epoch 33, loss: 2.302384\n",
      "Epoch 34, loss: 2.302373\n",
      "Epoch 35, loss: 2.302362\n",
      "Epoch 36, loss: 2.302351\n",
      "Epoch 37, loss: 2.302340\n",
      "Epoch 38, loss: 2.302329\n",
      "Epoch 39, loss: 2.302318\n",
      "Epoch 40, loss: 2.302307\n",
      "Epoch 41, loss: 2.302296\n",
      "Epoch 42, loss: 2.302285\n",
      "Epoch 43, loss: 2.302274\n",
      "Epoch 44, loss: 2.302263\n",
      "Epoch 45, loss: 2.302252\n",
      "Epoch 46, loss: 2.302241\n",
      "Epoch 47, loss: 2.302230\n",
      "Epoch 48, loss: 2.302219\n",
      "Epoch 49, loss: 2.302208\n",
      "Epoch 50, loss: 2.302198\n",
      "Epoch 51, loss: 2.302187\n",
      "Epoch 52, loss: 2.302176\n",
      "Epoch 53, loss: 2.302165\n",
      "Epoch 54, loss: 2.302154\n",
      "Epoch 55, loss: 2.302143\n",
      "Epoch 56, loss: 2.302132\n",
      "Epoch 57, loss: 2.302122\n",
      "Epoch 58, loss: 2.302111\n",
      "Epoch 59, loss: 2.302100\n",
      "Epoch 60, loss: 2.302089\n",
      "Epoch 61, loss: 2.302078\n",
      "Epoch 62, loss: 2.302068\n",
      "Epoch 63, loss: 2.302057\n",
      "Epoch 64, loss: 2.302046\n",
      "Epoch 65, loss: 2.302035\n",
      "Epoch 66, loss: 2.302024\n",
      "Epoch 67, loss: 2.302014\n",
      "Epoch 68, loss: 2.302003\n",
      "Epoch 69, loss: 2.301992\n",
      "Epoch 70, loss: 2.301981\n",
      "Epoch 71, loss: 2.301971\n",
      "Epoch 72, loss: 2.301960\n",
      "Epoch 73, loss: 2.301949\n",
      "Epoch 74, loss: 2.301939\n",
      "Epoch 75, loss: 2.301928\n",
      "Epoch 76, loss: 2.301917\n",
      "Epoch 77, loss: 2.301907\n",
      "Epoch 78, loss: 2.301896\n",
      "Epoch 79, loss: 2.301885\n",
      "Epoch 80, loss: 2.301875\n",
      "Epoch 81, loss: 2.301864\n",
      "Epoch 82, loss: 2.301853\n",
      "Epoch 83, loss: 2.301843\n",
      "Epoch 84, loss: 2.301832\n",
      "Epoch 85, loss: 2.301821\n",
      "Epoch 86, loss: 2.301811\n",
      "Epoch 87, loss: 2.301800\n",
      "Epoch 88, loss: 2.301789\n",
      "Epoch 89, loss: 2.301779\n",
      "Epoch 90, loss: 2.301768\n",
      "Epoch 91, loss: 2.301758\n",
      "Epoch 92, loss: 2.301747\n",
      "Epoch 93, loss: 2.301737\n",
      "Epoch 94, loss: 2.301726\n",
      "Epoch 95, loss: 2.301716\n",
      "Epoch 96, loss: 2.301705\n",
      "Epoch 97, loss: 2.301694\n",
      "Epoch 98, loss: 2.301684\n",
      "Epoch 99, loss: 2.301673\n",
      "Epoch 100, loss: 2.301663\n",
      "Epoch 101, loss: 2.301652\n",
      "Epoch 102, loss: 2.301642\n",
      "Epoch 103, loss: 2.301631\n",
      "Epoch 104, loss: 2.301621\n",
      "Epoch 105, loss: 2.301610\n",
      "Epoch 106, loss: 2.301600\n",
      "Epoch 107, loss: 2.301589\n",
      "Epoch 108, loss: 2.301579\n",
      "Epoch 109, loss: 2.301568\n",
      "Epoch 110, loss: 2.301558\n",
      "Epoch 111, loss: 2.301548\n",
      "Epoch 112, loss: 2.301537\n",
      "Epoch 113, loss: 2.301527\n",
      "Epoch 114, loss: 2.301516\n",
      "Epoch 115, loss: 2.301506\n",
      "Epoch 116, loss: 2.301496\n",
      "Epoch 117, loss: 2.301485\n",
      "Epoch 118, loss: 2.301475\n",
      "Epoch 119, loss: 2.301464\n",
      "Epoch 120, loss: 2.301454\n",
      "Epoch 121, loss: 2.301443\n",
      "Epoch 122, loss: 2.301433\n",
      "Epoch 123, loss: 2.301423\n",
      "Epoch 124, loss: 2.301412\n",
      "Epoch 125, loss: 2.301402\n",
      "Epoch 126, loss: 2.301391\n",
      "Epoch 127, loss: 2.301381\n",
      "Epoch 128, loss: 2.301371\n",
      "Epoch 129, loss: 2.301360\n",
      "Epoch 130, loss: 2.301350\n",
      "Epoch 131, loss: 2.301340\n",
      "Epoch 132, loss: 2.301330\n",
      "Epoch 133, loss: 2.301319\n",
      "Epoch 134, loss: 2.301309\n",
      "Epoch 135, loss: 2.301298\n",
      "Epoch 136, loss: 2.301288\n",
      "Epoch 137, loss: 2.301278\n",
      "Epoch 138, loss: 2.301268\n",
      "Epoch 139, loss: 2.301258\n",
      "Epoch 140, loss: 2.301247\n",
      "Epoch 141, loss: 2.301237\n",
      "Epoch 142, loss: 2.301227\n",
      "Epoch 143, loss: 2.301216\n",
      "Epoch 144, loss: 2.301206\n",
      "Epoch 145, loss: 2.301196\n",
      "Epoch 146, loss: 2.301186\n",
      "Epoch 147, loss: 2.301176\n",
      "Epoch 148, loss: 2.301165\n",
      "Epoch 149, loss: 2.301155\n",
      "Epoch 150, loss: 2.301145\n",
      "Epoch 151, loss: 2.301135\n",
      "Epoch 152, loss: 2.301124\n",
      "Epoch 153, loss: 2.301114\n",
      "Epoch 154, loss: 2.301104\n",
      "Epoch 155, loss: 2.301094\n",
      "Epoch 156, loss: 2.301084\n",
      "Epoch 157, loss: 2.301073\n",
      "Epoch 158, loss: 2.301063\n",
      "Epoch 159, loss: 2.301053\n",
      "Epoch 160, loss: 2.301043\n",
      "Epoch 161, loss: 2.301033\n",
      "Epoch 162, loss: 2.301023\n",
      "Epoch 163, loss: 2.301012\n",
      "Epoch 164, loss: 2.301002\n",
      "Epoch 165, loss: 2.300992\n",
      "Epoch 166, loss: 2.300982\n",
      "Epoch 167, loss: 2.300972\n",
      "Epoch 168, loss: 2.300962\n",
      "Epoch 169, loss: 2.300952\n",
      "Epoch 170, loss: 2.300942\n",
      "Epoch 171, loss: 2.300932\n",
      "Epoch 172, loss: 2.300921\n",
      "Epoch 173, loss: 2.300911\n",
      "Epoch 174, loss: 2.300901\n",
      "Epoch 175, loss: 2.300891\n",
      "Epoch 176, loss: 2.300881\n",
      "Epoch 177, loss: 2.300871\n",
      "Epoch 178, loss: 2.300861\n",
      "Epoch 179, loss: 2.300851\n",
      "Epoch 180, loss: 2.300841\n",
      "Epoch 181, loss: 2.300831\n",
      "Epoch 182, loss: 2.300821\n",
      "Epoch 183, loss: 2.300811\n",
      "Epoch 184, loss: 2.300801\n",
      "Epoch 185, loss: 2.300791\n",
      "Epoch 186, loss: 2.300781\n",
      "Epoch 187, loss: 2.300771\n",
      "Epoch 188, loss: 2.300761\n",
      "Epoch 189, loss: 2.300751\n",
      "Epoch 190, loss: 2.300740\n",
      "Epoch 191, loss: 2.300731\n",
      "Epoch 192, loss: 2.300721\n",
      "Epoch 193, loss: 2.300711\n",
      "Epoch 194, loss: 2.300701\n",
      "Epoch 195, loss: 2.300691\n",
      "Epoch 196, loss: 2.300681\n",
      "Epoch 197, loss: 2.300671\n",
      "Epoch 198, loss: 2.300661\n",
      "Epoch 199, loss: 2.300651\n",
      "Accuracy = 0.124000\n",
      "\n",
      "best validation accuracy achieved: 0.120000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        print('Learning rate = %f\\nReg strength = %f' % (lr, rs))\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=rs)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        print(\"Accuracy = %f\\n\" % accuracy)\n",
    "        if best_val_accuracy is None or best_val_accuracy < accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "        \n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.110000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
